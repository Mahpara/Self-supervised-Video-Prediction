{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ThatsAll.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3xEeBU7sAYaw",
        "r48oXNZZAdiP",
        "ouU0S0Uclye1",
        "vN3xPr8El8NS",
        "pmgGk4XSr4Jb",
        "NiwbrZKpFnsd",
        "j2qAKJv0wHfz",
        "YYhpWCVCkVAK",
        "IQkDANWrZTu8",
        "Y26NMEJ8Za7a",
        "YyoP_VEJAT7R",
        "KkKHWpgcAHlO",
        "G9Plt3kj_9r2"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xEeBU7sAYaw"
      },
      "source": [
        "### Pip installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngQpyJ-ecFeh",
        "outputId": "645d8422-45dc-4166-bdd7-29252f999e89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "!apt-get install --no-install-recommends ffmpeg && pip install ffmpeg scikit-video"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n",
            "Collecting ffmpeg\n",
            "  Downloading https://files.pythonhosted.org/packages/f0/cc/3b7408b8ecf7c1d20ad480c3eaed7619857bf1054b690226e906fdf14258/ffmpeg-1.4.tar.gz\n",
            "Collecting scikit-video\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/a6/c69cad508139a342810ae46e946ebb3256aa6e42f690d901bb68f50582e3/scikit_video-1.1.11-py2.py3-none-any.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 14.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-video) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from scikit-video) (1.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from scikit-video) (7.0.0)\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-cp36-none-any.whl size=6083 sha256=681034ddf4e20d5973b2c3f932cf3bffdc0228fd6b219cb1051e7c67b7f3fbf9\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/68/c3/a05a35f647ba871e5572b9bbfc0b95fd1c6637a2219f959e7a\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg, scikit-video\n",
            "Successfully installed ffmpeg-1.4 scikit-video-1.1.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r48oXNZZAdiP"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnBl1hkTAhtX"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as t\n",
        "import torchvision.models as pretrained_model\n",
        "import torchvision.datasets as ds\n",
        "from torchvision.utils import make_grid,save_image\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import torch.nn.init as init\n",
        "from torch.autograd import Variable\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import skvideo.io\n",
        "import os\n",
        "from glob import glob\n",
        "import random\n",
        "from math import exp\n",
        "import imageio\n",
        "import pickle\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUNhI6bR-dSf"
      },
      "source": [
        "torch.manual_seed(2805)\n",
        "np.random.seed(1310)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu1oO3CyALXp"
      },
      "source": [
        "## Dataset Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV9zD4zF77Sr"
      },
      "source": [
        "%matplotlib inline\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    #inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qyk4nHMZqvR"
      },
      "source": [
        "#### Extract Images after every 2 frames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FtDwOtScN-Q"
      },
      "source": [
        "'''\n",
        "UCF101 Dataset Preparation \n",
        "Source: https://github.com/xudejing/video-clip-order-prediction/blob/master/datasets/ucf101.py\n",
        "'''\n",
        "class UCF101Dataset(Dataset):\n",
        "    \"\"\"UCF101 dataset for recognition. The class index start from 0.\n",
        "\n",
        "    Args:\n",
        "        root_dir (string): Directory with videos and splits.\n",
        "        train (bool): train split or test split.\n",
        "        clip_len (int): number of frames in clip, \n",
        "        transforms_ (object): composed transforms which takes in PIL image and output tensors.\n",
        "        test_sample_num： number of clips sampled from a video. 1 for clip accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, annot_dir, clip_len, split='1', train=True, transforms_=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.annot_dir= annot_dir\n",
        "        self.clip_len = clip_len\n",
        "        self.split = split\n",
        "        self.train = train\n",
        "        self.transformation = transforms_\n",
        "        class_idx_path = os.path.join(self.annot_dir, 'classInd.txt')\n",
        "        self.class_idx2label = pd.read_csv(class_idx_path, header=None, sep=' ').set_index(0)[1]\n",
        "        self.class_label2idx = pd.read_csv(class_idx_path, header=None, sep=' ').set_index(1)[0]\n",
        "\n",
        "        if self.train:\n",
        "            train_split_path = os.path.join(self.annot_dir, 'trainlist0' + self.split + '.txt')\n",
        "            self.train_split = pd.read_csv(train_split_path, header=None, sep=' ')[0]\n",
        "        else:\n",
        "            test_split_path = os.path.join(self.annot_dir, 'testlist0' + self.split + '.txt')\n",
        "            self.test_split = pd.read_csv(test_split_path, header=None)[0]\n",
        "        print('Use split' + self.split)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.train:\n",
        "            return len(self.train_split)\n",
        "        else:\n",
        "            return len(self.test_split)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            clip (tensor): [time x channel  x height x width]\n",
        "            class_idx (tensor): class index, [0-100]\n",
        "        \"\"\"\n",
        "        if self.train:\n",
        "            videoname = self.train_split[idx]\n",
        "        else:\n",
        "            videoname = self.test_split[idx]\n",
        "        \n",
        "        class_idx = self.class_label2idx[videoname[:videoname.find('/')]]\n",
        "        filename = os.path.join(self.root_dir, videoname)\n",
        "        videodata = skvideo.io.vread(filename)\n",
        "        length, height, width, channel = videodata.shape\n",
        "        \n",
        "        if self.train:\n",
        "            frames_all = []\n",
        "            for i in np.linspace(self.clip_len/2,self.clip_len*2.5,self.clip_len):\n",
        "                frame_start = int(i)\n",
        "                frame = videodata[frame_start: frame_start + 1].squeeze(0)\n",
        "                if self.transformation:\n",
        "                    frame_transform = self.transformation(frame)  # tensor [C x H x W]\n",
        "                    frames_all.append(frame_transform)\n",
        "                else:\n",
        "                    frames_all.append(frame)\n",
        "            clip = torch.stack(frames_all)\n",
        "\n",
        "            return clip, torch.tensor(int(class_idx))\n",
        "        \n",
        "        else:\n",
        "            frames_all = []\n",
        "            for i in np.linspace(self.clip_len/2,self.clip_len*2.5,self.clip_len):\n",
        "                frame_start = int(i)\n",
        "                frame = videodata[frame_start: frame_start + 1].squeeze(0)\n",
        "                if self.transformation:\n",
        "                    frame_transform = self.transformation(frame)  # tensor [C x H x W]\n",
        "                    frames_all.append(frame_transform)\n",
        "                else:\n",
        "                    frames_all.append(frame)\n",
        "            clip = torch.stack(frames_all)\n",
        "\n",
        "            return clip, torch.tensor(int(class_idx))\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxQ8M9KMY36Q"
      },
      "source": [
        "#### Create Test Data GIF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQB35c9jPBfw"
      },
      "source": [
        "def gt_resizer(gt):\n",
        "    gt_resized=[]\n",
        "    for i in range(gt.size(1)):\n",
        "        frame = gt[:,i,:,:,:]\n",
        "        resized_frame = F.interpolate(frame,size=(112,112), mode = 'bilinear')\n",
        "        gt_resized.append(resized_frame)\n",
        "    gt_clip = torch.stack(gt_resized,1)\n",
        "    \n",
        "    return gt_clip\n",
        "\n",
        "\n",
        "def create_gif(clips, gif_file_name):\n",
        "    images = []\n",
        "    for i in range(3):\n",
        "        img_file_name = \"/content/drive/My Drive/CudaLAB/P R O J E C T/gifs/images/img.jpg\"\n",
        "        img = clips[i,:,:,:]\n",
        "        save_image(img,img_file_name)\n",
        "        images.append(imageio.imread(img_file_name))\n",
        "    \n",
        "    imageio.mimsave(gif_file_name,images,duration=0.5)\n",
        "    print(\"GIF has been saved.\")\n",
        "\n",
        "def gen_gif(ground_truth,batch_idx,prediction=False):\n",
        "    print(ground_truth.shape)\n",
        "    for i in range(ground_truth.size(0)):\n",
        "        clip = ground_truth[i,:,:,:,:]\n",
        "        if prediction:\n",
        "            gt_gif_saving_path = os.path.join(\"/content/drive/My Drive/CudaLAB/P R O J E C T/gifs_predicted\",\\\n",
        "                                          \"batch_%d_predicted_%d.gif\"%(batch_idx,i))\n",
        "        else: \n",
        "            gt_gif_saving_path = os.path.join(\"/content/drive/My Drive/CudaLAB/P R O J E C T/gifs\",\\\n",
        "                                          \"batch_%d_gt_%d.gif\"%(batch_idx,i))\n",
        "        create_gif(clip,gt_gif_saving_path)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouU0S0Uclye1"
      },
      "source": [
        "## Pre-trained Resnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILsrX1AVl3Dx"
      },
      "source": [
        "class PreTrainedResnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PreTrainedResnet,self).__init__()\n",
        "        self.resnet = pretrained_model.resnet18(pretrained= True)\n",
        "        self.set_forward_hook()\n",
        "    \n",
        "    def hook(self,module, input, output):\n",
        "        setattr(module, \"_value_hook\", output)\n",
        "\n",
        "    def set_forward_hook(self):\n",
        "        for name,module in self.resnet.named_modules():\n",
        "            for i in range(5):\n",
        "                if(name== 'layer'+str(i)):\n",
        "                    module.register_forward_hook(self.hook)\n",
        "        self.resnet.conv1.register_forward_hook(self.hook) # Register hook in that 7x7 conv at the beginning         \n",
        "\n",
        "    def forward(self,x):\n",
        "        res_y = self.resnet(x)\n",
        "        output_list =[]\n",
        "        ## Get the value from the hook of 7x7 conv layer\n",
        "        out_conv = self.resnet.conv1._value_hook\n",
        "        output_list.append(out_conv)\n",
        "        for name,module in self.resnet.named_modules():\n",
        "            for i in range(5):\n",
        "                if(name== 'layer'+str(i)):\n",
        "                    name_out = module._value_hook\n",
        "                    output_list.append(name_out)\n",
        "        return output_list\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN3xPr8El8NS"
      },
      "source": [
        "## Convolutional GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zaoo_xGwl6ch"
      },
      "source": [
        "# https://github.com/jacobkimmel/pytorch_convgru/blob/master/convgru.py\n",
        "class ConvGRUCell(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size):\n",
        "        super().__init__()\n",
        "        self.input_size  = in_ch\n",
        "        self.hidden_size = out_ch\n",
        "        self.kernelSize  = kernel_size\n",
        "        self.padding     = self.kernelSize // 2\n",
        "        \n",
        "        self.reset_gate  = nn.Conv2d(in_channels= self.input_size + self.hidden_size, \n",
        "                                     out_channels= self.hidden_size, kernel_size= self.kernelSize, padding= self.padding)\n",
        "        self.update_gate = nn.Conv2d(in_channels= self.input_size + self.hidden_size, \n",
        "                                     out_channels= self.hidden_size, kernel_size= self.kernelSize, padding= self.padding)\n",
        "        self.output_gate = nn.Conv2d(in_channels= self.input_size + self.hidden_size, \n",
        "                                     out_channels= self.hidden_size, kernel_size= self.kernelSize, padding= self.padding) # check input channel\n",
        "\n",
        "        ## Initialize weight by orthogonal initializer\n",
        "        init.orthogonal_(self.reset_gate.weight)\n",
        "        init.orthogonal_(self.update_gate.weight)\n",
        "        init.orthogonal_(self.output_gate.weight)\n",
        "\n",
        "        init.constant_(self.reset_gate.bias,0.)\n",
        "        init.constant_(self.update_gate.bias, 0.)\n",
        "        init.constant_(self.output_gate.bias, 0.)\n",
        "\n",
        "\n",
        "    def forward(self,input_data,prev_state=None):\n",
        "        self.batch_size = input_data.size(0)\n",
        "        self.height = input_data.size(2)\n",
        "        self.width = input_data.size(3)\n",
        "        if prev_state is None:\n",
        "            prev_state = Variable(torch.zeros((self.batch_size,self.hidden_size,self.height,self.width))).to(device)\n",
        "\n",
        "        # [batch, channel, height, width]\n",
        "        stacked_data = torch.cat([input_data, prev_state],dim =1).to(device)\n",
        "        update = torch.sigmoid(self.update_gate(stacked_data))\n",
        "        reset = torch.sigmoid(self.reset_gate(stacked_data))\n",
        "        candidate = torch.cat([input_data, prev_state * reset],dim=1)\n",
        "        output = torch.tanh(self.output_gate(candidate))\n",
        "        new_state = prev_state * (1 - update) + output * update\n",
        "\n",
        "        return new_state\n",
        "\n",
        "\n",
        "class ConvGruModel(nn.Module):\n",
        "    def __init__(self,input_ch, output_ch, kernel_sizes,num_layer):\n",
        "        super(ConvGruModel,self).__init__()\n",
        "        self.input_ch = input_ch\n",
        "        ## If output_ch and kernel_sizes are list, their length==num_layer\n",
        "        if type(output_ch) != list:\n",
        "            self.output_ch = [output_ch]* num_layer       \n",
        "            \n",
        "        else: \n",
        "            assert len(output_ch) == num_layer,' `output_ch` must have the same length as num_layer'\n",
        "            self.output_ch = output_ch\n",
        "\n",
        "        if type(kernel_sizes) != list:\n",
        "            self.kernel_sizes = [kernel_sizes] * num_layer\n",
        "        else:\n",
        "            assert len(kernel_sizes) == num_layer, ' `kernel_sizes` must have the same length as num_layer'\n",
        "            self.kernel_sizes = kernel_sizes\n",
        "\n",
        "        self.num_layer = num_layer\n",
        "        cells=[]\n",
        "        ## Create the ConvGRU Model ##\n",
        "        for i in range(self.num_layer):\n",
        "            if i == 0: \n",
        "                in_ch = self.input_ch\n",
        "            else:\n",
        "                in_ch = self.output_ch[i-1]\n",
        "            cell = ConvGRUCell(in_ch, self.output_ch[i], self.kernel_sizes[i]).to(device)\n",
        "            name = \"ConvGRUCell_\"+str(i).zfill(2)\n",
        "            setattr(self,name,cell)\n",
        "\n",
        "            cells.append(getattr(self,name))\n",
        "        self.cells = cells\n",
        "\n",
        "    def forward(self,x, hidden=None ):\n",
        "        if not hidden: \n",
        "            hidden =[None]* self.num_layer\n",
        "        updated_hidden = []\n",
        "        x_data = x\n",
        "\n",
        "        for i in range(self.num_layer):\n",
        "            cell = self.cells[i]\n",
        "            hidden_cell = hidden[i]\n",
        "            out_hidden_cell = cell(x_data,hidden_cell)\n",
        "            \n",
        "            updated_hidden.append(out_hidden_cell)\n",
        "            x_data = out_hidden_cell\n",
        "        \n",
        "        return updated_hidden\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmgGk4XSr4Jb"
      },
      "source": [
        "## Location Dependent Convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKX7RRkyAzPM"
      },
      "source": [
        "class LocationDependentConv(nn.Conv2d): #locationAwareConv2d\n",
        "    def __init__(self,gradient,width,height,in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
        "        super().__init__(in_channels, out_channels, kernel_size, stride=stride, padding=(kernel_size-2)//2+1, dilation=dilation, groups=groups, bias=bias)\n",
        "        self.locationBias=nn.Parameter(torch.zeros(width,height,3)).to(device) #w->width, h-> height\n",
        "        self.locationEncode=torch.tensor(torch.ones(width,height,3)).to(device)\n",
        "        if gradient:\n",
        "            for i in range(width):\n",
        "                self.locationEncode[i,:,1]=self.locationEncode[:,i,0]=(i/float(width-1))\n",
        "    def forward(self,inputs):\n",
        "        b=self.locationBias*self.locationEncode\n",
        "        out = super().forward(inputs)+b[:,:,0]+b[:,:,1]+b[:,:,2]\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiwbrZKpFnsd"
      },
      "source": [
        "## Intermediate Module of the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrtchXoIZnkw"
      },
      "source": [
        "def intermediate_phase(in_data,LDC=True):\n",
        "    if LDC: \n",
        "        loc_conv = LocationDependentConv(True, in_data.size(2),in_data.size(3), in_data.size(1),64,1).to(device)\n",
        "        out1 = loc_conv(in_data)\n",
        "        \n",
        "    else:\n",
        "        ##1x1 conv\n",
        "        conv1 = nn.Conv2d(in_channels=64, out_channels=64,kernel_size=1).to(device)\n",
        "        out1 = conv1(in_data)\n",
        "\n",
        "    conv_gru_model = ConvGruModel(input_ch= in_data.size(1), output_ch=64 , kernel_sizes=[3,5,7], num_layer=3).to(device)\n",
        "    \n",
        "    convgru_out =conv_gru_model(in_data)\n",
        "    final_cg_out = convgru_out[-1]  \n",
        "    output = torch.cat([out1,final_cg_out],dim=1)\n",
        "    \n",
        "    return output\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2qAKJv0wHfz"
      },
      "source": [
        "## Refinement Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2xYYHKNwGyt"
      },
      "source": [
        "class RefinementModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RefinementModule,self).__init__()\n",
        "        self.processing_layer= nn.Sequential(nn.ReLU(), nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=3,padding=1),\n",
        "                                             nn.PixelShuffle(2),\n",
        "                                             nn.Conv2d(in_channels=256,out_channels=64,kernel_size=3, padding=1))\n",
        "        \n",
        "    def forward(self,x):\n",
        "        out = self.processing_layer(x)\n",
        "        return out\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYhpWCVCkVAK"
      },
      "source": [
        "## Our Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT2sarnVkWdZ"
      },
      "source": [
        "class OurModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OurModel,self).__init__()\n",
        "        self.myResNet = PreTrainedResnet().to(device)\n",
        "        self.refine_res_out = RefinementModule().to(device)\n",
        "        self.decoder = nn.Sequential(nn.ReLU(), nn.BatchNorm2d(192),\n",
        "                                        nn.Conv2d(in_channels=192,out_channels=1024,kernel_size=3,padding=1),\n",
        "                                        nn.PixelShuffle(2), \n",
        "                                        nn.Conv2d(in_channels=256,out_channels=64,kernel_size=3, padding=1))\n",
        "        \n",
        "        \n",
        "        self.last = nn.Conv2d(in_channels=192,out_channels=3,kernel_size=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        model_output = []\n",
        "        #print(\"x.shape: \",x.shape) # torch.Size([8, 3, 3, 224, 224])\n",
        "\n",
        "        for i in range(x.size(1)): \n",
        "            x_new = x[:,i,:,:,:]\n",
        "            #print(\"Input shape: \", x_new.shape)   # torch.Size([3, 3, 224, 224])    \n",
        "            ### Pre-trained ResNet\n",
        "            res_output = self.myResNet(x_new)\n",
        "            \n",
        "            #res_output = res_output\n",
        "            r1,r2,r3,r4,r5 = res_output \n",
        "            #print(r1.shape)# r1.shape:  torch.Size([3, 64, 112, 112]), \n",
        "            # r2.shape:  torch.Size([3, 64, 56, 56])            \n",
        "            #r3.shape:  torch.Size([3, 128, 28, 28])\n",
        "            #r4.shape:  torch.Size([3, 256, 14, 14])\n",
        "            #r5.shape:  torch.Size([3, 512, 7, 7])\n",
        "            \n",
        "            ### Convolutional GRU\n",
        "            cg1 = intermediate_phase(r1,False) \n",
        "            cg2 = intermediate_phase(r2,False) \n",
        "            cg3 = intermediate_phase(r3) \n",
        "            cg4 = intermediate_phase(r4) \n",
        "\n",
        "            ### Decoder of the Network\n",
        "            tail1 = self.refine_res_out(r5)\n",
        "        \n",
        "            concat_data = torch.cat([tail1,cg4],dim=1)\n",
        "            tail2 = self.decoder(concat_data) \n",
        "        \n",
        "            concat_data2 = torch.cat([tail2,cg3],dim=1)     \n",
        "            tail3 = self.decoder(concat_data2) \n",
        "        \n",
        "            concat_data3 = torch.cat([tail3,cg2],dim=1) \n",
        "            tail4 = self.decoder(concat_data3) \n",
        "  \n",
        "            concat_data4 = torch.cat([tail4,cg1], dim=1)       \n",
        "            out = self.last(concat_data4)\n",
        "\n",
        "            model_output.append(out) \n",
        "        \n",
        "        prediction = torch.stack(model_output,1)\n",
        "        \n",
        "\n",
        "        return prediction\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQkDANWrZTu8"
      },
      "source": [
        "## UTILITIES\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD7FicqMNjFl"
      },
      "source": [
        "##### Loss Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztTzMim8ZTAB"
      },
      "source": [
        "#######     SSIM Loss    #######\n",
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
        "    return gauss/gauss.sum()\n",
        "\n",
        "def create_window(window_size, channel):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = torch.Tensor(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
        "    return window\n",
        "\n",
        "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
        "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
        "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1*mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
        "\n",
        "    C1 = 0.01**2\n",
        "    C2 = 0.03**2\n",
        "\n",
        "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
        "\n",
        "    if size_average:\n",
        "        return ssim_map.mean()\n",
        "    else:\n",
        "        return ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "class SSIM(torch.nn.Module):\n",
        "    def __init__(self, window_size = 11, size_average = True):\n",
        "        super(SSIM, self).__init__()\n",
        "        self.window_size = window_size\n",
        "        self.size_average = size_average\n",
        "        self.channel = 1\n",
        "        self.window = create_window(window_size, self.channel)\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        (_, channel, _, _) = img1.size()\n",
        "\n",
        "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
        "            window = self.window\n",
        "        else:\n",
        "            window = create_window(self.window_size, channel)\n",
        "            \n",
        "            if img1.is_cuda:\n",
        "                window = window.cuda(img1.get_device())\n",
        "            window = window.type_as(img1)\n",
        "            \n",
        "            self.window = window\n",
        "            self.channel = channel\n",
        "\n",
        "\n",
        "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
        "\n",
        "def ssim(img1, img2, window_size = 11, size_average = True):\n",
        "    (_, channel, _, _) = img1.size()\n",
        "    window = create_window(window_size, channel)\n",
        "    \n",
        "    if img1.is_cuda:\n",
        "        window = window.cuda(img1.get_device())\n",
        "    window = window.type_as(img1)\n",
        "    \n",
        "    return _ssim(img1, img2, window, window_size, channel, size_average)\n",
        "\n",
        "\n",
        "########        DSSIM Loss      ###########\n",
        "\n",
        "def dssim_loss(ground_truth, prediction): \n",
        "    dssim_val=0\n",
        "    ssim_loss = SSIM(window_size=11)\n",
        "    ## For Loop over every Batch\n",
        "    batch_size = ground_truth.size(0)\n",
        "    for i in range(batch_size):\n",
        "        ssim_val = ssim_loss(ground_truth[i,:,:,:,:], prediction[i,:,:,:,:])\n",
        "        dssim_loss = ((1-ssim_val)/2)\n",
        "        dssim_val += dssim_loss\n",
        "    dssim_val /= batch_size\n",
        "\n",
        "    return dssim_val\n",
        "\n",
        "def loss_manager(ground_truth,prediction,l1_weight=1, l2_weight=1, dssim_weight=1):\n",
        "    l1_loss = nn.L1Loss()\n",
        "    l1_loss_val = l1_loss(ground_truth,prediction) \n",
        "    l1_weighted_loss = l1_loss_val * l1_weight\n",
        "\n",
        "    l2_loss = nn.MSELoss()\n",
        "    l2_loss_val = l2_loss(ground_truth,prediction) \n",
        "    l2_weighted_loss = l2_loss_val * l2_weight\n",
        "\n",
        "    dssim_loss_val = dssim_loss(ground_truth, prediction) \n",
        "    dssim_weighted_loss = dssim_loss_val * dssim_weight\n",
        "\n",
        "    total_loss = l1_weighted_loss + l2_weighted_loss + dssim_weighted_loss\n",
        "\n",
        "    loss_dict = {'l1_loss': l1_loss_val.item(), 'l2_loss': l2_loss_val.item(),\n",
        "                 'dssim_loss':dssim_loss_val.item()}\n",
        "\n",
        "    return total_loss , loss_dict\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y26NMEJ8Za7a"
      },
      "source": [
        "## Functions :- train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pykfm0-AYwLm"
      },
      "source": [
        "def train(model,optimizer,training_data ,epoch):\n",
        "    model.train()\n",
        "    train_loss_f_path=\"/content/drive/My Drive/CudaLAB/P R O J E C T/loss/train_loss02.txt\"\n",
        "    loss_dict_keys = ['l1_loss','l2_loss','dssim_loss']\n",
        "    train_sep_loss = {key:[] for key in loss_dict_keys}\n",
        "    train_epoch_loss=[]\n",
        "\n",
        "    for i in range(epoch):\n",
        "        total = len(training_data.dataset)\n",
        "        total_loss = 0\n",
        "        for batch_idx,(clips,label) in enumerate(training_data):\n",
        "            optimizer.zero_grad()\n",
        "            clips = clips.to(device)            \n",
        "            #print(len(clips)) # batch_size -> 8\n",
        "            clips_input = clips[:,0:3,:,:,:] ## Take first 3 frames as input\n",
        "            #print(\"clips_input.shape: \",clips_input.shape) # torch.Size([8, 3, 3, 224, 224])\n",
        "            clips_truth = clips[:,1:4,:,:,:] ## Select last 3 frames as ground truth to compare\n",
        "            #print(\"clips_truth.shape: \", clips_truth.shape) # torch.Size([8, 3, 3, 224, 224])\n",
        "            ground_truth =gt_resizer(clips_truth).to(device)\n",
        "            \n",
        "            out = our_model(clips_input)\n",
        "            loss,loss_dict = loss_manager(ground_truth,out,l1_weight=1, l2_weight=0.5, dssim_weight=2)\n",
        "\n",
        "            ## Append the 'train_sep_loss' dict\n",
        "            train_sep_loss['l1_loss'].append(loss_dict['l1_loss'])\n",
        "            train_sep_loss['l2_loss'].append(loss_dict['l2_loss'])\n",
        "            train_sep_loss['dssim_loss'].append(loss_dict['dssim_loss'])\n",
        "\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            if (batch_idx% 100==0):\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                i, batch_idx, int(total/len(clips)),\n",
        "                100. * batch_idx / len(training_data), loss.item() / len(clips)))\n",
        "        \n",
        "        epoch_loss = total_loss / batch_idx\n",
        "        print(\"Epoch: {}, Loss: {:6f}\".format(i, epoch_loss))  \n",
        "        train_epoch_loss.append(epoch_loss)\n",
        "        \n",
        "        ## Save Loss for Plotting in a txt file [in \"append\" mode]\n",
        "        with open(train_loss_f_path,'a') as f:\n",
        "            f.write(str(epoch_loss)+ '\\n')\n",
        "\n",
        "        ## Save Checkpoint\n",
        "        model_saving_path = os.path.join(\"/content/drive/My Drive/CudaLAB/P R O J E C T/new_saved_models\",\"model_0_%d.pth\" % (i+22))\n",
        "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),'loss': loss.item() / len(clips),\n",
        "                    }, model_saving_path)\n",
        "\n",
        "    return train_epoch_loss, train_sep_loss\n",
        "\n",
        "def test(model, testloader):\n",
        "    model.eval()\n",
        "    total = len(testloader.dataset)\n",
        "    print(\"Total Number of examples in Test Dataset: \", total)\n",
        "    loss_dict_keys = ['l1_loss','l2_loss','dssim_loss']\n",
        "    test_sep_loss = {key:[] for key in loss_dict_keys}\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx,data in enumerate(testloader):\n",
        "            clips, labels = data\n",
        "            clips = clips.to(device)  \n",
        "            clips_input = clips[:,0:3,:,:,:] ## Take first 3 frames as input\n",
        "            #print(\"clips_input.shape: \",clips_input.shape)\n",
        "            clips_truth = clips[:,1:4,:,:,:] ## Select last 3 frames as ground truth to compare\n",
        "            #print(\"clips_truth.shape: \", clips_truth.shape)\n",
        "            ground_truth =gt_resizer(clips_truth).to(device)\n",
        "            predicted_clip = our_model(clips_input)\n",
        "            test_loss,test_loss_dict = loss_manager(ground_truth,predicted_clip,l1_weight=1, l2_weight=0.5, dssim_weight=2)\n",
        "            ## Append the 'test_sep_loss' dict\n",
        "            test_sep_loss['l1_loss'].append(test_loss_dict['l1_loss'])\n",
        "            test_sep_loss['l2_loss'].append(test_loss_dict['l2_loss'])\n",
        "            test_sep_loss['dssim_loss'].append(test_loss_dict['dssim_loss'])\n",
        "            \n",
        "            total_loss += test_loss.item()\n",
        "            \n",
        "            if (batch_idx ==73):\n",
        "                gen_gif(ground_truth,batch_idx)\n",
        "                gen_gif(predicted_clip,batch_idx,True)\n",
        "                print(\"Now you can terminate the program\")\n",
        "            \n",
        "        test_all_loss=total_loss / batch_idx\n",
        "    \n",
        "    return test_all_loss,test_sep_loss\n",
        "\n",
        "   "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyoP_VEJAT7R"
      },
      "source": [
        "## Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RSaDArF0CRP",
        "outputId": "ead2b150-56ed-4e6c-baae-a3f0c26a3580",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "train_transformations = t.Compose([t.ToPILImage(),\n",
        "                             t.Resize((224,224)),\n",
        "                             t.RandomHorizontalFlip(),\n",
        "                             t.ToTensor(),\n",
        "                             t.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "test_transformations = t.Compose([t.ToPILImage(),\n",
        "                             t.Resize((224,224)),\n",
        "                             t.ToTensor(),\n",
        "                             t.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "data_dir = \"/content/drive/My Drive/CudaLAB/P R O J E C T/UCF-101\"\n",
        "annotation_dir = \"/content/drive/My Drive/CudaLAB/P R O J E C T/data/ucfLabel\"\n",
        "bs = 16\n",
        "train_dataset = UCF101Dataset(data_dir,annotation_dir,4,transforms_=train_transformations)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=bs,num_workers=32)\n",
        "test_dataset = UCF101Dataset(data_dir,annotation_dir,4,train=False, transforms_=test_transformations)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=bs,num_workers=32)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use split1\n",
            "Use split1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkKHWpgcAHlO"
      },
      "source": [
        "## Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ChYp3yQ7UGr"
      },
      "source": [
        "'''\n",
        "our_model = OurModel().to(device)\n",
        "lr = 0.0001\n",
        "optimizer = optim.Adam(our_model.parameters(),lr)\n",
        "loss, loss_dict = train(our_model, optimizer, train_dataloader,epoch=5)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9Plt3kj_9r2"
      },
      "source": [
        "#### Training from saved Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PqKpARi_89_",
        "outputId": "0b31ab17-55ba-4965-e6ef-434eba78b5f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "saved_model_path= \"/content/drive/My Drive/CudaLAB/P R O J E C T/new_saved_models/model_0_21.pth\"\n",
        "\n",
        "our_model = OurModel().to(device)\n",
        "lr = 0.0001\n",
        "optimizer = optim.Adam(our_model.parameters(),lr)\n",
        "checkpoint = torch.load(saved_model_path)\n",
        "our_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "train_loss, train_loss_dict = train(our_model, optimizer, train_dataloader,epoch=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/596 (0%)]\tLoss: 0.087440\n",
            "Train Epoch: 0 [100/596 (17%)]\tLoss: 0.094410\n",
            "Train Epoch: 0 [200/596 (34%)]\tLoss: 0.086545\n",
            "Train Epoch: 0 [300/596 (50%)]\tLoss: 0.079408\n",
            "Train Epoch: 0 [400/596 (67%)]\tLoss: 0.086280\n",
            "Train Epoch: 0 [500/596 (84%)]\tLoss: 0.063352\n",
            "Epoch: 0, Loss: 1.390611\n",
            "Train Epoch: 1 [0/596 (0%)]\tLoss: 0.089528\n",
            "Train Epoch: 1 [100/596 (17%)]\tLoss: 0.098814\n",
            "Train Epoch: 1 [200/596 (34%)]\tLoss: 0.084046\n",
            "Train Epoch: 1 [300/596 (50%)]\tLoss: 0.079847\n",
            "Train Epoch: 1 [400/596 (67%)]\tLoss: 0.093567\n",
            "Train Epoch: 1 [500/596 (84%)]\tLoss: 0.057338\n",
            "Epoch: 1, Loss: 1.381736\n",
            "Train Epoch: 2 [0/596 (0%)]\tLoss: 0.096498\n",
            "Train Epoch: 2 [100/596 (17%)]\tLoss: 0.089347\n",
            "Train Epoch: 2 [200/596 (34%)]\tLoss: 0.089131\n",
            "Train Epoch: 2 [300/596 (50%)]\tLoss: 0.076191\n",
            "Train Epoch: 2 [400/596 (67%)]\tLoss: 0.079412\n",
            "Train Epoch: 2 [500/596 (84%)]\tLoss: 0.061564\n",
            "Epoch: 2, Loss: 1.383765\n",
            "Train Epoch: 3 [0/596 (0%)]\tLoss: 0.087266\n",
            "Train Epoch: 3 [100/596 (17%)]\tLoss: 0.095351\n",
            "Train Epoch: 3 [200/596 (34%)]\tLoss: 0.087038\n",
            "Train Epoch: 3 [300/596 (50%)]\tLoss: 0.076564\n",
            "Train Epoch: 3 [400/596 (67%)]\tLoss: 0.089008\n",
            "Train Epoch: 3 [500/596 (84%)]\tLoss: 0.060537\n",
            "Epoch: 3, Loss: 1.376829\n",
            "Train Epoch: 4 [0/596 (0%)]\tLoss: 0.092893\n",
            "Train Epoch: 4 [100/596 (17%)]\tLoss: 0.089233\n",
            "Train Epoch: 4 [200/596 (34%)]\tLoss: 0.089256\n",
            "Train Epoch: 4 [300/596 (50%)]\tLoss: 0.078482\n",
            "Train Epoch: 4 [400/596 (67%)]\tLoss: 0.091283\n",
            "Train Epoch: 4 [500/596 (84%)]\tLoss: 0.055691\n",
            "Epoch: 4, Loss: 1.375644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZx7KtAEArVH",
        "outputId": "e483f4de-1af3-4dc8-d2e6-5a90f9e3fe95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(train_loss)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.3906108499973413, 1.3817360404153798, 1.383764588392821, 1.3768290333299829, 1.375643541148845]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN11AH9aAZid"
      },
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F-XS8V28KUj"
      },
      "source": [
        "#### Inference from saved Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqKTfy_T1GO5",
        "outputId": "44f5543f-b4a3-475e-94c6-cd5b351aa454",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "our_model = OurModel().to(device)\n",
        "saved_model_path= \"/content/drive/My Drive/CudaLAB/P R O J E C T/new_saved_models/model_0_26.pth\"\n",
        "checkpoint = torch.load(saved_model_path)\n",
        "our_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "test_all_loss,test_sep_loss = test(our_model,test_dataloader)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Number of examples in Test Dataset:  3783\n",
            "torch.Size([16, 3, 3, 112, 112])\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "torch.Size([16, 3, 3, 112, 112])\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "GIF has been saved.\n",
            "Now you can terminate the program\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twsnaxG31Pdi",
        "outputId": "c9dc32f3-e294-4e73-fe81-420dea01d23c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "mean_sep_loss= {k: sum(v) / float(len(v)) for k, v in test_sep_loss.items()}\n",
        "print(\"Average of each loss: \",mean_sep_loss)\n",
        "\n",
        "print(\"Test Loss: \",test_all_loss)\n",
        "print(\"Weighted Average of Test Loss: \", test_all_loss/3)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average of each loss:  {'l1_loss': 0.9134166155183365, 'l2_loss': 1.1786907830821813, 'dssim_loss': 0.43543484467494337}\n",
            "Test Loss:  2.383689466169325\n",
            "Weighted Average of Test Loss:  0.794563155389775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-px11tvfAV09"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}